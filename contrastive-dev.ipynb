{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e07322cb",
   "metadata": {},
   "source": [
    "# Contrastive Learning development\n",
    "## Method: SimCLR\n",
    "\n",
    "- reference: https://arxiv.org/pdf/2002.05709.pdf  \n",
    "- use: training on pairs of angle-jittered thickness-sphere images\n",
    "- TO DO: add more testing for loss function; all other components seem to work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef3c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Note on development:\n",
    "    https://stackoverflow.com/questions/66065272/customizing-the-batch-with-specific-elements\n",
    "    was a useful reference for designing the custom batch_sampler.\n",
    "\n",
    "\"\"\"\n",
    "import user\n",
    "from user.data import CustomImageDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from typing import Union  # useful for function annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88459949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_products(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" Computes all inner products of two batches of tensors.\n",
    "        Given x_bi, y_cj (batch dimension first), returns\n",
    "            s_bc = x_bi y_ci, inner product over vector dim.\n",
    "    \"\"\"\n",
    "    assert x.shape == y.shape  # inputs must be compatible\n",
    "    xnorm = torch.norm(x, dim=-1)  # norm is Euclidean by default\n",
    "    ynorm = torch.norm(y, dim=-1)\n",
    "    denom = torch.outer(xnorm, ynorm)\n",
    "    # d = xnorm.shape[0] # get appropriate dimension for outer product\n",
    "    # denom = xnorm.reshape(d, 1) @ ynorm.reshape(1, d)\n",
    "    xdoty = x @ y.T\n",
    "    \n",
    "    return torch.div(xdoty, denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c8b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\" One hidden-layer MLP for the projection heads of network.\n",
    "    \"\"\"\n",
    "    def __init__(self, nin, nmid, nout, nonlin=None):\n",
    "        super().__init__()\n",
    "        if nonlin is None:\n",
    "            nonlin = F.elu\n",
    "        self.l1 = nn.Linear(nin, nmid)\n",
    "        self.l2 = nn.Linear(nmid, nout)\n",
    "        self.nonlin = nonlin\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.nonlin(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af6ad493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(inn: torch.Tensor, tau: Union[float, torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\" Contrastive loss.\n",
    "        Inputs:\n",
    "            inn: s_ij, inner products of vector batch.\n",
    "            tau: temperature parameter\n",
    "    \"\"\"\n",
    "    d = inn.shape[0]\n",
    "    assert d % 2 == 0  # must be complete number of pairs\n",
    "    innexp = torch.exp(inn/tau)  # complete matrix of all pairwise exps\n",
    "    denoms = torch.sum(innexp, dim=-1) - torch.diagonal(inn, 0)\n",
    "    \n",
    "    # compute all losses between (positive) pairs: 2x,2x+1 and 2x+1,x\n",
    "    left_nums = torch.diagonal(innexp, 1)  # above diagonal; innexp_i,i+1\n",
    "    left_loss = -torch.log(left_nums[::2]/denoms[:-1][::2])\n",
    "    right_nums = torch.diagonal(innexp, -1)  # below diagonal; innexp_i+1,i\n",
    "    right_loss = -torch.log(right_nums[::2]/denoms[:-1][::2])\n",
    "    total_loss = (1/d) * torch.sum(right_loss + left_loss) # normalize by 1/d\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3941f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "048680ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_inner = inner_products(output, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea8a4714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_inner.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3db87240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1568, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(output_inner, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a6f25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  dataset loading / processing\n",
    "\n",
    "img_dir = '../data/fsa-data/images/images-thickness-sphere-processed'\n",
    "label_dir = '../data/labels'\n",
    "label_file = label_dir + '/age_thickness_sphere_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f02e8a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bd84ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paired_labels(df, nsamples=10, savepath=None):\n",
    "    \"\"\" Takes a non-paired label file and creates\n",
    "        a file of uniform labels, grouped by subject\n",
    "        with the same number of samples per subject.\n",
    "    \"\"\"\n",
    "    if isinstance(df, str):\n",
    "        labels = pd.read_csv(df)\n",
    "    labels = df\n",
    "    # create new labels file, in preparation for custom batch sampler\n",
    "    labels['Subject'] = labels['Filename'].apply(lambda s: s[:16])\n",
    "    counts = labels.groupby('Subject', as_index=False).count()\n",
    "    counts.columns = list(counts.columns)[:-1] + ['Count']\n",
    "    label_counts = pd.merge(labels, counts[['Subject', 'Count']], on='Subject', how='left')\n",
    "    labels_uniform = label_counts[label_counts.Count==nsamples]\n",
    "    # very important to sort data in labels so that indices are in order\n",
    "    labels_uniform_sorted = labels_uniform.sort_values('Filename')\n",
    "    \n",
    "    #  optionally saves a .csv of modified labels\n",
    "    if savepath is not None:\n",
    "        labels_uniform_sorted.to_csv(savepath, index=None)\n",
    "    \n",
    "    return labels_uniform_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c83fae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelfile = label_dir + '/age_thickness_sphere_paired_labels.csv'\n",
    "\n",
    "labels_exist = True\n",
    "if not labels_exist:\n",
    "    labels_unif = create_paired_labels(labels)\n",
    "    labels_unif.to_csv(labelfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80a262ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv(labelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c7b8394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Age</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-NDARAA075AMK.fsa.lh.thickness--62.10--1.83...</td>\n",
       "      <td>7</td>\n",
       "      <td>sub-NDARAA075AMK</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-NDARAA075AMK.fsa.lh.thickness--64.32--1.43...</td>\n",
       "      <td>7</td>\n",
       "      <td>sub-NDARAA075AMK</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-NDARAA075AMK.fsa.lh.thickness--64.88--3.30...</td>\n",
       "      <td>7</td>\n",
       "      <td>sub-NDARAA075AMK</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-NDARAA075AMK.fsa.lh.thickness--64.95-2.36-...</td>\n",
       "      <td>7</td>\n",
       "      <td>sub-NDARAA075AMK</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-NDARAA075AMK.fsa.lh.thickness--64.97-3.92-...</td>\n",
       "      <td>7</td>\n",
       "      <td>sub-NDARAA075AMK</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sub-NDARAA075AMK.fsa.lh.thickness--65.86-2.87-...</td>\n",
       "      <td>7</td>\n",
       "      <td>sub-NDARAA075AMK</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sub-NDARAA075AMK.fsa.lh.thickness--66.10--1.62...</td>\n",
       "      <td>7</td>\n",
       "      <td>sub-NDARAA075AMK</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sub-NDARAA075AMK.fsa.lh.thickness--67.74--1.54...</td>\n",
       "      <td>7</td>\n",
       "      <td>sub-NDARAA075AMK</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sub-NDARAA075AMK.fsa.lh.thickness--67.89-3.39-...</td>\n",
       "      <td>7</td>\n",
       "      <td>sub-NDARAA075AMK</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sub-NDARAA075AMK.fsa.lh.thickness--68.23--2.24...</td>\n",
       "      <td>7</td>\n",
       "      <td>sub-NDARAA075AMK</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sub-NDARAA504CRN.fsa.lh.thickness--62.05--1.72...</td>\n",
       "      <td>9</td>\n",
       "      <td>sub-NDARAA504CRN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sub-NDARAA504CRN.fsa.lh.thickness--62.42-0.42-...</td>\n",
       "      <td>9</td>\n",
       "      <td>sub-NDARAA504CRN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sub-NDARAA504CRN.fsa.lh.thickness--62.61-1.68-...</td>\n",
       "      <td>9</td>\n",
       "      <td>sub-NDARAA504CRN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sub-NDARAA504CRN.fsa.lh.thickness--62.77-3.77-...</td>\n",
       "      <td>9</td>\n",
       "      <td>sub-NDARAA504CRN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sub-NDARAA504CRN.fsa.lh.thickness--62.79--4.93...</td>\n",
       "      <td>9</td>\n",
       "      <td>sub-NDARAA504CRN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sub-NDARAA504CRN.fsa.lh.thickness--63.18--2.12...</td>\n",
       "      <td>9</td>\n",
       "      <td>sub-NDARAA504CRN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sub-NDARAA504CRN.fsa.lh.thickness--64.54--1.86...</td>\n",
       "      <td>9</td>\n",
       "      <td>sub-NDARAA504CRN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sub-NDARAA504CRN.fsa.lh.thickness--68.04--2.84...</td>\n",
       "      <td>9</td>\n",
       "      <td>sub-NDARAA504CRN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sub-NDARAA504CRN.fsa.lh.thickness--69.43-0.92-...</td>\n",
       "      <td>9</td>\n",
       "      <td>sub-NDARAA504CRN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sub-NDARAA504CRN.fsa.lh.thickness--69.71-2.17-...</td>\n",
       "      <td>9</td>\n",
       "      <td>sub-NDARAA504CRN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Filename  Age           Subject  \\\n",
       "0   sub-NDARAA075AMK.fsa.lh.thickness--62.10--1.83...    7  sub-NDARAA075AMK   \n",
       "1   sub-NDARAA075AMK.fsa.lh.thickness--64.32--1.43...    7  sub-NDARAA075AMK   \n",
       "2   sub-NDARAA075AMK.fsa.lh.thickness--64.88--3.30...    7  sub-NDARAA075AMK   \n",
       "3   sub-NDARAA075AMK.fsa.lh.thickness--64.95-2.36-...    7  sub-NDARAA075AMK   \n",
       "4   sub-NDARAA075AMK.fsa.lh.thickness--64.97-3.92-...    7  sub-NDARAA075AMK   \n",
       "5   sub-NDARAA075AMK.fsa.lh.thickness--65.86-2.87-...    7  sub-NDARAA075AMK   \n",
       "6   sub-NDARAA075AMK.fsa.lh.thickness--66.10--1.62...    7  sub-NDARAA075AMK   \n",
       "7   sub-NDARAA075AMK.fsa.lh.thickness--67.74--1.54...    7  sub-NDARAA075AMK   \n",
       "8   sub-NDARAA075AMK.fsa.lh.thickness--67.89-3.39-...    7  sub-NDARAA075AMK   \n",
       "9   sub-NDARAA075AMK.fsa.lh.thickness--68.23--2.24...    7  sub-NDARAA075AMK   \n",
       "10  sub-NDARAA504CRN.fsa.lh.thickness--62.05--1.72...    9  sub-NDARAA504CRN   \n",
       "11  sub-NDARAA504CRN.fsa.lh.thickness--62.42-0.42-...    9  sub-NDARAA504CRN   \n",
       "12  sub-NDARAA504CRN.fsa.lh.thickness--62.61-1.68-...    9  sub-NDARAA504CRN   \n",
       "13  sub-NDARAA504CRN.fsa.lh.thickness--62.77-3.77-...    9  sub-NDARAA504CRN   \n",
       "14  sub-NDARAA504CRN.fsa.lh.thickness--62.79--4.93...    9  sub-NDARAA504CRN   \n",
       "15  sub-NDARAA504CRN.fsa.lh.thickness--63.18--2.12...    9  sub-NDARAA504CRN   \n",
       "16  sub-NDARAA504CRN.fsa.lh.thickness--64.54--1.86...    9  sub-NDARAA504CRN   \n",
       "17  sub-NDARAA504CRN.fsa.lh.thickness--68.04--2.84...    9  sub-NDARAA504CRN   \n",
       "18  sub-NDARAA504CRN.fsa.lh.thickness--69.43-0.92-...    9  sub-NDARAA504CRN   \n",
       "19  sub-NDARAA504CRN.fsa.lh.thickness--69.71-2.17-...    9  sub-NDARAA504CRN   \n",
       "\n",
       "    Count  \n",
       "0      10  \n",
       "1      10  \n",
       "2      10  \n",
       "3      10  \n",
       "4      10  \n",
       "5      10  \n",
       "6      10  \n",
       "7      10  \n",
       "8      10  \n",
       "9      10  \n",
       "10     10  \n",
       "11     10  \n",
       "12     10  \n",
       "13     10  \n",
       "14     10  \n",
       "15     10  \n",
       "16     10  \n",
       "17     10  \n",
       "18     10  \n",
       "19     10  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8e12ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that here, img_dir need not change; the labelfile\n",
    "# simply specifies a subset of images\n",
    "from user.data import ToFloat\n",
    "\n",
    "ds = CustomImageDataset(labelfile, img_dir, transform=transforms.ConvertImageDtype(torch.float32), target_transform=ToFloat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1902b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedBatchSampler(torch.utils.data.Sampler):\n",
    "    \"\"\" Custom batch sampler, to sample (either sequentially\n",
    "        or randomly) in pairs. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, datasource, nxs, ntrans, batchsize, random=False):\n",
    "        super().__init__(datasource)\n",
    "        self.nx = nxs\n",
    "        self.nt = ntrans\n",
    "        self.nb = batchsize\n",
    "        self.random = random\n",
    "        if not self.random:\n",
    "            #  these properties are only necessary for sequential sampling\n",
    "            self.nextx = 0\n",
    "            self.pair_ind = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\" Creates index sample of 2 * batchsize,\n",
    "            batchsize number of pairs. Different depending on whether\n",
    "            sampling is random or sequential.\n",
    "        \"\"\"\n",
    "        return self._create_batch_iterator()\n",
    "    \n",
    "    def _create_batch_iterator(self):\n",
    "        return PairedBatchSampler._batch_iterator(self)\n",
    "\n",
    "    class _batch_iterator():\n",
    "        \"\"\" Helper class to create a batch iterator. This is necessary\n",
    "            because batch_sampler (arg of DataLoader) expects an iterator\n",
    "            over batches, not merely over elements of a batch.\n",
    "        \"\"\"\n",
    "        #  Note: could accomplish the random sampling by using _pair()\n",
    "        def __init__(self, outer):\n",
    "            self.outer = outer\n",
    "        \n",
    "        def __iter__(self):\n",
    "            return self\n",
    "        \n",
    "        def __next__(self):\n",
    "            if self.outer.random:\n",
    "                pinds = []\n",
    "                for i in range(self.outer.nb):\n",
    "                    pinds.append(np.random.choice(list(range(self.outer.nt)), size=2, replace=False))\n",
    "                pinds = np.array(pinds)\n",
    "                xinds = np.random.randint(0, self.outer.nx, self.outer.nb)\n",
    "                inds = (pinds.T + self.outer.nt * xinds).T # must multiply xinds by nt to yield actual ind\n",
    "                inds = inds.reshape(self.outer.nb * 2)\n",
    "\n",
    "            if not self.outer.random:\n",
    "                xinds = list(range(self.outer.nextx, self.outer.nextx + self.outer.nb))\n",
    "                xinds = [i % self.outer.nx for i in xinds]  # wrap back to beginning of db\n",
    "                if self.outer.nextx > self.outer.nx - self.outer.nb:  # change pair after all subjects\n",
    "                    self.outer.pair_ind += 1\n",
    "                i, j = self.outer._pairs(self.outer.pair_ind)\n",
    "                xinds  = [self.outer.nt * xi for xi in xinds]\n",
    "                eveninds = [xi + i for xi in xinds]\n",
    "                oddinds = [xi + j for xi in xinds]\n",
    "                inds = [v for pair in zip(eveninds, oddinds) for v in pair]\n",
    "\n",
    "                self.outer.nextx = (self.outer.nextx + self.outer.nb) % self.outer.nx\n",
    "\n",
    "            return inds\n",
    "\n",
    "    def _pairs(self, ind, base=None):\n",
    "        \"\"\" Helper function to uniquely associate indices of a valid pair\n",
    "            with the numbers from 0 to <number of possible pairs> - 1.\n",
    "            If called without base specified, base will be set to\n",
    "            self.nt as this is the primary use case.\n",
    "        \"\"\"\n",
    "        if base is None:\n",
    "            base = self.nt\n",
    "        #  get pair of indices    \n",
    "        i = ind // (base - 1)\n",
    "        j = ind % (base - 1)\n",
    "        if j >= i:\n",
    "            j += 1\n",
    "        return i, j   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0388308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15510"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45f8bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = PairedBatchSampler(ds, len(ds)//10, 10, 32, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eead279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_sampler=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b3fccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c82289",
   "metadata": {},
   "source": [
    "# Test entire SimCLR pipeline\n",
    "\n",
    "by training on pairs of angle-jittered images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b1f8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from user.models import ConvPoolBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "120e743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(nn.Module):\n",
    "    \"\"\" Network architecture for contrastive learning.\n",
    "        200 x 200 inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pool='Conv'):\n",
    "        super().__init__()\n",
    "        self.block1 = ConvPoolBlock(1, 32, 64, pool=pool)\n",
    "        self.block2 = ConvPoolBlock(64, 128, 128, pool=pool)\n",
    "        self.block3 = ConvPoolBlock(128, 128, 128, pool=pool)\n",
    "        self.block4 = ConvPoolBlock(128, 128, 64, pool=pool)\n",
    "        self.fc1 = nn.Linear(6400, 100)\n",
    "        self.ph = ProjectionHead(100, 32, 8)\n",
    "        # second-to-last hidden layer\n",
    "        # self.z = torch.autograd.Variable(torch.zeros(100, dtype=torch.float), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        # self.z = x  # save second-to-last hidden layer's values in network Variable.\n",
    "        # x = F.elu(self.fc2(x))\n",
    "        # x = F.elu(self.fc3(x))\n",
    "        x = self.ph(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeab6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimCLR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2870b060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimCLR(\n",
       "  (block1): ConvPoolBlock(\n",
       "    (c1): Conv2d(1, 32, kernel_size=(4, 4), stride=(1, 1), padding=same)\n",
       "    (c2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  )\n",
       "  (block2): ConvPoolBlock(\n",
       "    (c1): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=same)\n",
       "    (c2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "  )\n",
       "  (block3): ConvPoolBlock(\n",
       "    (c1): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1), padding=same)\n",
       "    (c2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "  )\n",
       "  (block4): ConvPoolBlock(\n",
       "    (c1): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1), padding=same)\n",
       "    (c2): Conv2d(128, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  )\n",
       "  (fc1): Linear(in_features=6400, out_features=100, bias=True)\n",
       "  (ph): ProjectionHead(\n",
       "    (l1): Linear(in_features=100, out_features=32, bias=True)\n",
       "    (l2): Linear(in_features=32, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = user.utils.get_device()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "704ba7ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─ConvPoolBlock: 1-1                     [-1, 64, 99, 99]          --\n",
      "|    └─Conv2d: 2-1                       [-1, 32, 200, 200]        544\n",
      "|    └─Conv2d: 2-2                       [-1, 64, 99, 99]          32,832\n",
      "├─ConvPoolBlock: 1-2                     [-1, 128, 48, 48]         --\n",
      "|    └─Conv2d: 2-3                       [-1, 128, 99, 99]         131,200\n",
      "|    └─Conv2d: 2-4                       [-1, 128, 48, 48]         262,272\n",
      "├─ConvPoolBlock: 1-3                     [-1, 128, 23, 23]         --\n",
      "|    └─Conv2d: 2-5                       [-1, 128, 48, 48]         262,272\n",
      "|    └─Conv2d: 2-6                       [-1, 128, 23, 23]         262,272\n",
      "├─ConvPoolBlock: 1-4                     [-1, 64, 10, 10]          --\n",
      "|    └─Conv2d: 2-7                       [-1, 128, 23, 23]         262,272\n",
      "|    └─Conv2d: 2-8                       [-1, 64, 10, 10]          131,136\n",
      "├─Linear: 1-5                            [-1, 100]                 640,100\n",
      "├─ProjectionHead: 1-6                    [-1, 8]                   --\n",
      "|    └─Linear: 2-9                       [-1, 32]                  3,232\n",
      "|    └─Linear: 2-10                      [-1, 8]                   264\n",
      "==========================================================================================\n",
      "Total params: 1,988,396\n",
      "Trainable params: 1,988,396\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 3.13\n",
      "==========================================================================================\n",
      "Input size (MB): 0.15\n",
      "Forward/backward pass size (MB): 29.71\n",
      "Params size (MB): 7.59\n",
      "Estimated Total Size (MB): 37.44\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/groups/jyeatman/software/anaconda3/envs/torch-ni-ny/lib/python3.9/site-packages/torch/nn/modules/conv.py:442: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272204863/work/aten/src/ATen/native/Convolution.cpp:647.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─ConvPoolBlock: 1-1                     [-1, 64, 99, 99]          --\n",
       "|    └─Conv2d: 2-1                       [-1, 32, 200, 200]        544\n",
       "|    └─Conv2d: 2-2                       [-1, 64, 99, 99]          32,832\n",
       "├─ConvPoolBlock: 1-2                     [-1, 128, 48, 48]         --\n",
       "|    └─Conv2d: 2-3                       [-1, 128, 99, 99]         131,200\n",
       "|    └─Conv2d: 2-4                       [-1, 128, 48, 48]         262,272\n",
       "├─ConvPoolBlock: 1-3                     [-1, 128, 23, 23]         --\n",
       "|    └─Conv2d: 2-5                       [-1, 128, 48, 48]         262,272\n",
       "|    └─Conv2d: 2-6                       [-1, 128, 23, 23]         262,272\n",
       "├─ConvPoolBlock: 1-4                     [-1, 64, 10, 10]          --\n",
       "|    └─Conv2d: 2-7                       [-1, 128, 23, 23]         262,272\n",
       "|    └─Conv2d: 2-8                       [-1, 64, 10, 10]          131,136\n",
       "├─Linear: 1-5                            [-1, 100]                 640,100\n",
       "├─ProjectionHead: 1-6                    [-1, 8]                   --\n",
       "|    └─Linear: 2-9                       [-1, 32]                  3,232\n",
       "|    └─Linear: 2-10                      [-1, 8]                   264\n",
       "==========================================================================================\n",
       "Total params: 1,988,396\n",
       "Trainable params: 1,988,396\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 3.13\n",
       "==========================================================================================\n",
       "Input size (MB): 0.15\n",
       "Forward/backward pass size (MB): 29.71\n",
       "Params size (MB): 7.59\n",
       "Estimated Total Size (MB): 37.44\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net, (1, 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d8e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "326e7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc3a6eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcec5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test by training to predict angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee3b8c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "625ada4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "92490738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_and_inner(X, tau=0.5):\n",
    "    \"\"\" Loss function for contrastive learning.\n",
    "        Computes the inner products of pairs.\n",
    "        Default temperature tau of 0.5 is taken from original paper.\"\"\"\n",
    "    inn = inner_products(X, X)\n",
    "    return loss(inn, tau=tau)\n",
    "\n",
    "def loss_and_inner_dummy(X, _):\n",
    "    return loss_and_inner(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "259b98e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1568, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_and_inner_dummy(output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e66410c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = loss_and_inner_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bab1c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from user.utils import get_device\n",
    "\n",
    "def contrastive_train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    device = get_device()\n",
    "    size = len(dataloader.dataset)\n",
    "    # ensure model is set to training mode in case previously in evaluation mode\n",
    "    model.train()\n",
    "    for batch, (X_cpu, y_cpu) in enumerate(dataloader):\n",
    "        batch_size = X_cpu.shape[0]\n",
    "        # Compute prediction and loss\n",
    "        X = X_cpu.to(device) # put both model and data on gpu (if available)\n",
    "        y = y_cpu.to(device)\n",
    "        output = model(X)\n",
    "        # this is required for models with multiple returns; by assumption,\n",
    "        # the first return of a tuple is the prediction; can only squeeze a tensor\n",
    "        if isinstance(output, tuple):\n",
    "            output = tuple([torch.squeeze(o) for o in output])\n",
    "        \n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "        # in case dataloader has cyclic (infinite) enumeration\n",
    "        if batch_size * batch > size:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08d8eb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.156766  [    0/15510]\n",
      "loss: 4.151641  [  640/15510]\n",
      "loss: 4.155554  [ 1280/15510]\n",
      "loss: 3.558662  [ 1920/15510]\n",
      "loss: 3.697443  [ 2560/15510]\n",
      "loss: 3.498979  [ 3200/15510]\n",
      "loss: 3.535395  [ 3840/15510]\n",
      "loss: 3.272998  [ 4480/15510]\n",
      "loss: 3.101564  [ 5120/15510]\n",
      "loss: 3.004882  [ 5760/15510]\n",
      "loss: 3.105434  [ 6400/15510]\n",
      "loss: 3.027913  [ 7040/15510]\n",
      "loss: 2.913542  [ 7680/15510]\n",
      "loss: 2.799510  [ 8320/15510]\n",
      "loss: 3.060619  [ 8960/15510]\n",
      "loss: 2.887210  [ 9600/15510]\n",
      "loss: 2.793036  [10240/15510]\n",
      "loss: 2.829901  [10880/15510]\n",
      "loss: 2.869812  [11520/15510]\n",
      "loss: 2.894574  [12160/15510]\n",
      "loss: 2.815442  [12800/15510]\n",
      "loss: 2.758851  [13440/15510]\n",
      "loss: 2.940223  [14080/15510]\n",
      "loss: 2.785467  [14720/15510]\n",
      "loss: 2.772412  [15360/15510]\n",
      "loss: 3.013589  [    0/15510]\n",
      "loss: 2.887372  [  640/15510]\n",
      "loss: 2.845021  [ 1280/15510]\n",
      "loss: 2.793487  [ 1920/15510]\n",
      "loss: 2.880816  [ 2560/15510]\n",
      "loss: 2.810629  [ 3200/15510]\n",
      "loss: 2.756248  [ 3840/15510]\n",
      "loss: 2.793505  [ 4480/15510]\n",
      "loss: 2.787173  [ 5120/15510]\n",
      "loss: 2.906445  [ 5760/15510]\n",
      "loss: 2.754999  [ 6400/15510]\n",
      "loss: 2.749282  [ 7040/15510]\n",
      "loss: 2.780032  [ 7680/15510]\n",
      "loss: 2.816274  [ 8320/15510]\n",
      "loss: 2.687294  [ 8960/15510]\n",
      "loss: 2.754716  [ 9600/15510]\n",
      "loss: 2.690897  [10240/15510]\n",
      "loss: 2.841833  [10880/15510]\n",
      "loss: 2.821277  [11520/15510]\n",
      "loss: 2.711663  [12160/15510]\n",
      "loss: 2.784001  [12800/15510]\n",
      "loss: 2.690751  [13440/15510]\n",
      "loss: 2.876402  [14080/15510]\n",
      "loss: 2.647589  [14720/15510]\n",
      "loss: 2.692905  [15360/15510]\n",
      "loss: 2.709480  [    0/15510]\n",
      "loss: 2.656251  [  640/15510]\n",
      "loss: 2.813812  [ 1280/15510]\n",
      "loss: 2.683975  [ 1920/15510]\n",
      "loss: 2.676010  [ 2560/15510]\n",
      "loss: 2.679996  [ 3200/15510]\n",
      "loss: 2.677336  [ 3840/15510]\n",
      "loss: 2.714756  [ 4480/15510]\n",
      "loss: 2.699343  [ 5120/15510]\n",
      "loss: 2.672077  [ 5760/15510]\n",
      "loss: 2.628770  [ 6400/15510]\n",
      "loss: 2.747648  [ 7040/15510]\n",
      "loss: 2.683516  [ 7680/15510]\n",
      "loss: 2.756978  [ 8320/15510]\n",
      "loss: 2.816499  [ 8960/15510]\n",
      "loss: 2.747556  [ 9600/15510]\n",
      "loss: 2.695013  [10240/15510]\n",
      "loss: 2.670446  [10880/15510]\n",
      "loss: 2.633947  [11520/15510]\n",
      "loss: 2.685170  [12160/15510]\n",
      "loss: 2.703440  [12800/15510]\n",
      "loss: 2.655882  [13440/15510]\n",
      "loss: 2.672776  [14080/15510]\n",
      "loss: 2.663557  [14720/15510]\n",
      "loss: 2.648359  [15360/15510]\n",
      "loss: 2.683254  [    0/15510]\n",
      "loss: 2.712433  [  640/15510]\n",
      "loss: 2.667781  [ 1280/15510]\n",
      "loss: 2.644034  [ 1920/15510]\n",
      "loss: 2.673914  [ 2560/15510]\n",
      "loss: 2.718796  [ 3200/15510]\n",
      "loss: 2.684304  [ 3840/15510]\n",
      "loss: 2.737304  [ 4480/15510]\n",
      "loss: 2.637803  [ 5120/15510]\n",
      "loss: 2.652660  [ 5760/15510]\n",
      "loss: 2.665215  [ 6400/15510]\n",
      "loss: 2.710230  [ 7040/15510]\n",
      "loss: 2.641427  [ 7680/15510]\n",
      "loss: 2.727815  [ 8320/15510]\n",
      "loss: 2.692457  [ 8960/15510]\n",
      "loss: 2.686196  [ 9600/15510]\n",
      "loss: 2.641095  [10240/15510]\n",
      "loss: 2.669237  [10880/15510]\n",
      "loss: 2.683633  [11520/15510]\n",
      "loss: 2.668576  [12160/15510]\n",
      "loss: 2.613872  [12800/15510]\n",
      "loss: 2.657019  [13440/15510]\n",
      "loss: 2.740955  [14080/15510]\n",
      "loss: 2.658641  [14720/15510]\n",
      "loss: 2.702919  [15360/15510]\n",
      "loss: 2.645482  [    0/15510]\n",
      "loss: 2.641941  [  640/15510]\n",
      "loss: 2.714379  [ 1280/15510]\n",
      "loss: 2.597620  [ 1920/15510]\n",
      "loss: 2.617250  [ 2560/15510]\n",
      "loss: 2.708528  [ 3200/15510]\n",
      "loss: 2.599738  [ 3840/15510]\n",
      "loss: 2.655116  [ 4480/15510]\n",
      "loss: 2.639256  [ 5120/15510]\n",
      "loss: 2.576727  [ 5760/15510]\n",
      "loss: 2.615933  [ 6400/15510]\n",
      "loss: 2.564959  [ 7040/15510]\n",
      "loss: 2.683728  [ 7680/15510]\n",
      "loss: 2.655679  [ 8320/15510]\n",
      "loss: 2.619796  [ 8960/15510]\n",
      "loss: 2.604304  [ 9600/15510]\n",
      "loss: 2.693511  [10240/15510]\n",
      "loss: 2.576400  [10880/15510]\n",
      "loss: 2.629265  [11520/15510]\n",
      "loss: 2.719135  [12160/15510]\n",
      "loss: 2.629574  [12800/15510]\n",
      "loss: 2.649381  [13440/15510]\n",
      "loss: 2.578134  [14080/15510]\n",
      "loss: 2.588565  [14720/15510]\n",
      "loss: 2.626760  [15360/15510]\n",
      "loss: 2.614359  [    0/15510]\n",
      "loss: 2.708838  [  640/15510]\n",
      "loss: 2.604340  [ 1280/15510]\n",
      "loss: 2.630867  [ 1920/15510]\n",
      "loss: 2.578046  [ 2560/15510]\n",
      "loss: 2.636282  [ 3200/15510]\n",
      "loss: 2.574640  [ 3840/15510]\n",
      "loss: 2.622530  [ 4480/15510]\n",
      "loss: 2.745909  [ 5120/15510]\n",
      "loss: 2.614668  [ 5760/15510]\n",
      "loss: 2.679090  [ 6400/15510]\n",
      "loss: 2.658549  [ 7040/15510]\n",
      "loss: 2.671166  [ 7680/15510]\n",
      "loss: 2.647261  [ 8320/15510]\n",
      "loss: 2.731890  [ 8960/15510]\n",
      "loss: 2.652994  [ 9600/15510]\n",
      "loss: 2.580287  [10240/15510]\n",
      "loss: 2.605037  [10880/15510]\n",
      "loss: 2.607874  [11520/15510]\n",
      "loss: 2.628532  [12160/15510]\n",
      "loss: 2.730219  [12800/15510]\n",
      "loss: 2.614883  [13440/15510]\n",
      "loss: 2.608339  [14080/15510]\n",
      "loss: 2.726259  [14720/15510]\n",
      "loss: 2.654387  [15360/15510]\n",
      "loss: 2.701121  [    0/15510]\n",
      "loss: 2.621677  [  640/15510]\n",
      "loss: 2.664298  [ 1280/15510]\n",
      "loss: 2.707377  [ 1920/15510]\n",
      "loss: 2.680610  [ 2560/15510]\n",
      "loss: 2.668930  [ 3200/15510]\n",
      "loss: 2.603268  [ 3840/15510]\n",
      "loss: 2.615500  [ 4480/15510]\n",
      "loss: 2.694687  [ 5120/15510]\n",
      "loss: 2.669673  [ 5760/15510]\n",
      "loss: 2.589941  [ 6400/15510]\n",
      "loss: 2.560151  [ 7040/15510]\n",
      "loss: 2.644047  [ 7680/15510]\n",
      "loss: 2.593270  [ 8320/15510]\n",
      "loss: 2.713146  [ 8960/15510]\n",
      "loss: 2.563614  [ 9600/15510]\n",
      "loss: 2.672698  [10240/15510]\n",
      "loss: 2.641934  [10880/15510]\n",
      "loss: 2.562369  [11520/15510]\n",
      "loss: 2.667154  [12160/15510]\n",
      "loss: 2.666870  [12800/15510]\n",
      "loss: 2.581050  [13440/15510]\n",
      "loss: 2.606919  [14080/15510]\n",
      "loss: 2.606603  [14720/15510]\n",
      "loss: 2.596174  [15360/15510]\n",
      "loss: 2.699660  [    0/15510]\n",
      "loss: 2.604380  [  640/15510]\n",
      "loss: 2.613338  [ 1280/15510]\n",
      "loss: 2.651641  [ 1920/15510]\n",
      "loss: 2.605708  [ 2560/15510]\n",
      "loss: 2.666408  [ 3200/15510]\n",
      "loss: 2.621644  [ 3840/15510]\n",
      "loss: 2.643533  [ 4480/15510]\n",
      "loss: 2.570901  [ 5120/15510]\n",
      "loss: 2.628418  [ 5760/15510]\n",
      "loss: 2.589969  [ 6400/15510]\n",
      "loss: 2.715787  [ 7040/15510]\n",
      "loss: 2.641928  [ 7680/15510]\n",
      "loss: 2.577317  [ 8320/15510]\n",
      "loss: 2.616150  [ 8960/15510]\n",
      "loss: 2.692708  [ 9600/15510]\n",
      "loss: 2.584706  [10240/15510]\n",
      "loss: 2.701267  [10880/15510]\n",
      "loss: 2.622333  [11520/15510]\n",
      "loss: 2.610776  [12160/15510]\n",
      "loss: 2.603742  [12800/15510]\n",
      "loss: 2.586310  [13440/15510]\n",
      "loss: 2.558487  [14080/15510]\n",
      "loss: 2.657398  [14720/15510]\n",
      "loss: 2.640313  [15360/15510]\n",
      "loss: 2.654191  [    0/15510]\n",
      "loss: 2.678098  [  640/15510]\n",
      "loss: 2.603879  [ 1280/15510]\n",
      "loss: 2.604552  [ 1920/15510]\n",
      "loss: 2.592399  [ 2560/15510]\n",
      "loss: 2.659922  [ 3200/15510]\n",
      "loss: 2.631406  [ 3840/15510]\n",
      "loss: 2.674835  [ 4480/15510]\n",
      "loss: 2.634086  [ 5120/15510]\n",
      "loss: 2.625322  [ 5760/15510]\n",
      "loss: 2.597672  [ 6400/15510]\n",
      "loss: 2.562156  [ 7040/15510]\n",
      "loss: 2.601255  [ 7680/15510]\n",
      "loss: 2.551027  [ 8320/15510]\n",
      "loss: 2.547903  [ 8960/15510]\n",
      "loss: 2.613833  [ 9600/15510]\n",
      "loss: 2.575159  [10240/15510]\n",
      "loss: 2.568228  [10880/15510]\n",
      "loss: 2.620404  [11520/15510]\n",
      "loss: 2.580248  [12160/15510]\n",
      "loss: 2.652078  [12800/15510]\n",
      "loss: 2.664969  [13440/15510]\n",
      "loss: 2.676343  [14080/15510]\n",
      "loss: 2.649247  [14720/15510]\n",
      "loss: 2.577281  [15360/15510]\n",
      "loss: 2.566400  [    0/15510]\n",
      "loss: 2.661013  [  640/15510]\n",
      "loss: 2.682233  [ 1280/15510]\n",
      "loss: 2.615481  [ 1920/15510]\n",
      "loss: 2.573386  [ 2560/15510]\n",
      "loss: 2.599905  [ 3200/15510]\n",
      "loss: 2.611248  [ 3840/15510]\n",
      "loss: 2.648314  [ 4480/15510]\n",
      "loss: 2.587634  [ 5120/15510]\n",
      "loss: 2.629932  [ 5760/15510]\n",
      "loss: 2.610479  [ 6400/15510]\n",
      "loss: 2.586863  [ 7040/15510]\n",
      "loss: 2.583240  [ 7680/15510]\n",
      "loss: 2.629366  [ 8320/15510]\n",
      "loss: 2.586804  [ 8960/15510]\n",
      "loss: 2.588794  [ 9600/15510]\n",
      "loss: 2.644044  [10240/15510]\n",
      "loss: 2.608911  [10880/15510]\n",
      "loss: 2.646430  [11520/15510]\n",
      "loss: 2.642238  [12160/15510]\n",
      "loss: 2.657302  [12800/15510]\n",
      "loss: 2.653162  [13440/15510]\n",
      "loss: 2.566047  [14080/15510]\n",
      "loss: 2.640382  [14720/15510]\n",
      "loss: 2.699110  [15360/15510]\n",
      "loss: 2.635285  [    0/15510]\n",
      "loss: 2.588743  [  640/15510]\n",
      "loss: 2.573184  [ 1280/15510]\n",
      "loss: 2.585263  [ 1920/15510]\n",
      "loss: 2.654777  [ 2560/15510]\n",
      "loss: 2.668774  [ 3200/15510]\n",
      "loss: 2.613460  [ 3840/15510]\n",
      "loss: 2.569711  [ 4480/15510]\n",
      "loss: 2.596327  [ 5120/15510]\n",
      "loss: 2.626382  [ 5760/15510]\n",
      "loss: 2.657528  [ 6400/15510]\n",
      "loss: 2.573482  [ 7040/15510]\n",
      "loss: 2.569524  [ 7680/15510]\n",
      "loss: 2.583721  [ 8320/15510]\n",
      "loss: 2.578518  [ 8960/15510]\n",
      "loss: 2.651358  [ 9600/15510]\n",
      "loss: 2.568614  [10240/15510]\n",
      "loss: 2.606256  [10880/15510]\n",
      "loss: 2.593406  [11520/15510]\n",
      "loss: 2.564948  [12160/15510]\n",
      "loss: 2.659662  [12800/15510]\n",
      "loss: 2.617312  [13440/15510]\n",
      "loss: 2.642080  [14080/15510]\n",
      "loss: 2.697681  [14720/15510]\n",
      "loss: 2.630629  [15360/15510]\n",
      "loss: 2.588795  [    0/15510]\n",
      "loss: 2.607213  [  640/15510]\n",
      "loss: 2.589022  [ 1280/15510]\n",
      "loss: 2.612907  [ 1920/15510]\n",
      "loss: 2.682780  [ 2560/15510]\n",
      "loss: 2.637507  [ 3200/15510]\n",
      "loss: 2.571555  [ 3840/15510]\n",
      "loss: 2.591694  [ 4480/15510]\n",
      "loss: 2.628530  [ 5120/15510]\n",
      "loss: 2.591767  [ 5760/15510]\n",
      "loss: 2.619000  [ 6400/15510]\n",
      "loss: 2.637908  [ 7040/15510]\n",
      "loss: 2.577510  [ 7680/15510]\n",
      "loss: 2.515896  [ 8320/15510]\n",
      "loss: 2.684436  [ 8960/15510]\n",
      "loss: 2.633874  [ 9600/15510]\n",
      "loss: 2.622864  [10240/15510]\n",
      "loss: 2.563374  [10880/15510]\n",
      "loss: 2.654891  [11520/15510]\n",
      "loss: 2.588270  [12160/15510]\n",
      "loss: 2.613803  [12800/15510]\n",
      "loss: 2.626652  [13440/15510]\n",
      "loss: 2.626118  [14080/15510]\n",
      "loss: 2.591989  [14720/15510]\n",
      "loss: 2.581624  [15360/15510]\n",
      "loss: 2.724303  [    0/15510]\n",
      "loss: 2.570079  [  640/15510]\n",
      "loss: 2.598184  [ 1280/15510]\n",
      "loss: 2.579855  [ 1920/15510]\n",
      "loss: 2.639198  [ 2560/15510]\n",
      "loss: 2.587223  [ 3200/15510]\n",
      "loss: 2.673106  [ 3840/15510]\n",
      "loss: 2.581930  [ 4480/15510]\n",
      "loss: 2.620885  [ 5120/15510]\n",
      "loss: 2.574154  [ 5760/15510]\n",
      "loss: 2.606238  [ 6400/15510]\n",
      "loss: 2.605092  [ 7040/15510]\n",
      "loss: 2.586586  [ 7680/15510]\n",
      "loss: 2.558089  [ 8320/15510]\n",
      "loss: 2.637326  [ 8960/15510]\n",
      "loss: 2.614262  [ 9600/15510]\n",
      "loss: 2.620491  [10240/15510]\n",
      "loss: 2.643350  [10880/15510]\n",
      "loss: 2.592367  [11520/15510]\n",
      "loss: 2.564291  [12160/15510]\n",
      "loss: 2.593200  [12800/15510]\n",
      "loss: 2.537928  [13440/15510]\n",
      "loss: 2.582682  [14080/15510]\n",
      "loss: 2.629296  [14720/15510]\n",
      "loss: 2.671399  [15360/15510]\n",
      "loss: 2.561935  [    0/15510]\n",
      "loss: 2.613809  [  640/15510]\n",
      "loss: 2.603794  [ 1280/15510]\n",
      "loss: 2.648652  [ 1920/15510]\n",
      "loss: 2.560444  [ 2560/15510]\n",
      "loss: 2.607145  [ 3200/15510]\n",
      "loss: 2.616233  [ 3840/15510]\n",
      "loss: 2.635737  [ 4480/15510]\n",
      "loss: 2.610602  [ 5120/15510]\n",
      "loss: 2.602068  [ 5760/15510]\n",
      "loss: 2.608195  [ 6400/15510]\n",
      "loss: 2.529062  [ 7040/15510]\n",
      "loss: 2.622627  [ 7680/15510]\n",
      "loss: 2.677806  [ 8320/15510]\n",
      "loss: 2.663818  [ 8960/15510]\n",
      "loss: 2.577398  [ 9600/15510]\n",
      "loss: 2.581835  [10240/15510]\n",
      "loss: 2.578333  [10880/15510]\n",
      "loss: 2.551953  [11520/15510]\n",
      "loss: 2.614808  [12160/15510]\n",
      "loss: 2.584860  [12800/15510]\n",
      "loss: 2.625355  [13440/15510]\n",
      "loss: 2.560808  [14080/15510]\n",
      "loss: 2.631502  [14720/15510]\n",
      "loss: 2.557526  [15360/15510]\n",
      "loss: 2.557314  [    0/15510]\n",
      "loss: 2.560862  [  640/15510]\n",
      "loss: 2.553164  [ 1280/15510]\n",
      "loss: 2.662224  [ 1920/15510]\n",
      "loss: 2.627763  [ 2560/15510]\n",
      "loss: 2.566724  [ 3200/15510]\n",
      "loss: 2.572771  [ 3840/15510]\n",
      "loss: 2.622149  [ 4480/15510]\n",
      "loss: 2.625696  [ 5120/15510]\n",
      "loss: 2.662635  [ 5760/15510]\n",
      "loss: 2.593626  [ 6400/15510]\n",
      "loss: 2.576730  [ 7040/15510]\n",
      "loss: 2.574310  [ 7680/15510]\n",
      "loss: 2.559778  [ 8320/15510]\n",
      "loss: 2.567404  [ 8960/15510]\n",
      "loss: 2.568373  [ 9600/15510]\n",
      "loss: 2.609933  [10240/15510]\n",
      "loss: 2.586595  [10880/15510]\n",
      "loss: 2.609792  [11520/15510]\n",
      "loss: 2.632979  [12160/15510]\n",
      "loss: 2.578739  [12800/15510]\n",
      "loss: 2.568348  [13440/15510]\n",
      "loss: 2.586751  [14080/15510]\n",
      "loss: 2.633771  [14720/15510]\n",
      "loss: 2.614445  [15360/15510]\n",
      "loss: 2.548700  [    0/15510]\n",
      "loss: 2.597136  [  640/15510]\n",
      "loss: 2.569158  [ 1280/15510]\n",
      "loss: 2.560939  [ 1920/15510]\n",
      "loss: 2.542924  [ 2560/15510]\n",
      "loss: 2.564051  [ 3200/15510]\n",
      "loss: 2.651180  [ 3840/15510]\n",
      "loss: 2.604978  [ 4480/15510]\n",
      "loss: 2.599177  [ 5120/15510]\n",
      "loss: 2.592216  [ 5760/15510]\n",
      "loss: 2.649173  [ 6400/15510]\n",
      "loss: 2.608833  [ 7040/15510]\n",
      "loss: 2.612286  [ 7680/15510]\n",
      "loss: 2.578351  [ 8320/15510]\n",
      "loss: 2.565324  [ 8960/15510]\n",
      "loss: 2.588554  [ 9600/15510]\n",
      "loss: 2.593388  [10240/15510]\n",
      "loss: 2.518136  [10880/15510]\n",
      "loss: 2.593393  [11520/15510]\n",
      "loss: 2.544831  [12160/15510]\n",
      "loss: 2.646679  [12800/15510]\n",
      "loss: 2.581079  [13440/15510]\n",
      "loss: 2.601745  [14080/15510]\n",
      "loss: 2.600962  [14720/15510]\n",
      "loss: 2.572238  [15360/15510]\n",
      "loss: 2.580176  [    0/15510]\n",
      "loss: 2.531206  [  640/15510]\n",
      "loss: 2.587839  [ 1280/15510]\n",
      "loss: 2.666094  [ 1920/15510]\n",
      "loss: 2.590786  [ 2560/15510]\n",
      "loss: 2.585310  [ 3200/15510]\n",
      "loss: 2.587708  [ 3840/15510]\n",
      "loss: 2.595966  [ 4480/15510]\n",
      "loss: 2.572778  [ 5120/15510]\n",
      "loss: 2.601074  [ 5760/15510]\n",
      "loss: 2.698230  [ 6400/15510]\n",
      "loss: 2.543591  [ 7040/15510]\n",
      "loss: 2.595675  [ 7680/15510]\n",
      "loss: 2.647305  [ 8320/15510]\n",
      "loss: 2.587723  [ 8960/15510]\n",
      "loss: 2.550634  [ 9600/15510]\n",
      "loss: 2.638325  [10240/15510]\n",
      "loss: 2.619846  [10880/15510]\n",
      "loss: 2.595942  [11520/15510]\n",
      "loss: 2.545592  [12160/15510]\n",
      "loss: 2.626931  [12800/15510]\n",
      "loss: 2.570425  [13440/15510]\n",
      "loss: 2.542613  [14080/15510]\n",
      "loss: 2.603308  [14720/15510]\n",
      "loss: 2.571497  [15360/15510]\n",
      "loss: 2.577186  [    0/15510]\n",
      "loss: 2.514411  [  640/15510]\n",
      "loss: 2.627636  [ 1280/15510]\n",
      "loss: 2.667459  [ 1920/15510]\n",
      "loss: 2.642088  [ 2560/15510]\n",
      "loss: 2.541124  [ 3200/15510]\n",
      "loss: 2.606131  [ 3840/15510]\n",
      "loss: 2.599224  [ 4480/15510]\n",
      "loss: 2.614384  [ 5120/15510]\n",
      "loss: 2.555318  [ 5760/15510]\n",
      "loss: 2.686757  [ 6400/15510]\n",
      "loss: 2.537622  [ 7040/15510]\n",
      "loss: 2.604344  [ 7680/15510]\n",
      "loss: 2.634723  [ 8320/15510]\n",
      "loss: 2.557602  [ 8960/15510]\n",
      "loss: 2.544897  [ 9600/15510]\n",
      "loss: 2.590021  [10240/15510]\n",
      "loss: 2.654065  [10880/15510]\n",
      "loss: 2.611583  [11520/15510]\n",
      "loss: 2.578897  [12160/15510]\n",
      "loss: 2.588297  [12800/15510]\n",
      "loss: 2.636685  [13440/15510]\n",
      "loss: 2.612487  [14080/15510]\n",
      "loss: 2.646337  [14720/15510]\n",
      "loss: 2.572830  [15360/15510]\n",
      "loss: 2.651143  [    0/15510]\n",
      "loss: 2.551302  [  640/15510]\n",
      "loss: 2.586058  [ 1280/15510]\n",
      "loss: 2.562607  [ 1920/15510]\n",
      "loss: 2.653887  [ 2560/15510]\n",
      "loss: 2.577648  [ 3200/15510]\n",
      "loss: 2.555714  [ 3840/15510]\n",
      "loss: 2.584633  [ 4480/15510]\n",
      "loss: 2.585901  [ 5120/15510]\n",
      "loss: 2.524418  [ 5760/15510]\n",
      "loss: 2.603586  [ 6400/15510]\n",
      "loss: 2.547925  [ 7040/15510]\n",
      "loss: 2.595769  [ 7680/15510]\n",
      "loss: 2.594856  [ 8320/15510]\n",
      "loss: 2.593813  [ 8960/15510]\n",
      "loss: 2.610787  [ 9600/15510]\n",
      "loss: 2.592751  [10240/15510]\n",
      "loss: 2.613177  [10880/15510]\n",
      "loss: 2.564745  [11520/15510]\n",
      "loss: 2.592011  [12160/15510]\n",
      "loss: 2.636543  [12800/15510]\n",
      "loss: 2.556414  [13440/15510]\n",
      "loss: 2.591110  [14080/15510]\n",
      "loss: 2.567561  [14720/15510]\n",
      "loss: 2.568896  [15360/15510]\n",
      "loss: 2.591093  [    0/15510]\n",
      "loss: 2.571867  [  640/15510]\n",
      "loss: 2.554483  [ 1280/15510]\n",
      "loss: 2.634970  [ 1920/15510]\n",
      "loss: 2.560491  [ 2560/15510]\n",
      "loss: 2.595546  [ 3200/15510]\n",
      "loss: 2.640361  [ 3840/15510]\n",
      "loss: 2.524764  [ 4480/15510]\n",
      "loss: 2.541243  [ 5120/15510]\n",
      "loss: 2.597357  [ 5760/15510]\n",
      "loss: 2.584407  [ 6400/15510]\n",
      "loss: 2.610919  [ 7040/15510]\n",
      "loss: 2.596561  [ 7680/15510]\n",
      "loss: 2.621893  [ 8320/15510]\n",
      "loss: 2.539146  [ 8960/15510]\n",
      "loss: 2.537646  [ 9600/15510]\n",
      "loss: 2.590799  [10240/15510]\n",
      "loss: 2.601418  [10880/15510]\n",
      "loss: 2.584120  [11520/15510]\n",
      "loss: 2.593629  [12160/15510]\n",
      "loss: 2.554156  [12800/15510]\n",
      "loss: 2.539670  [13440/15510]\n",
      "loss: 2.600364  [14080/15510]\n",
      "loss: 2.550742  [14720/15510]\n",
      "loss: 2.553495  [15360/15510]\n",
      "loss: 2.536993  [    0/15510]\n",
      "loss: 2.585298  [  640/15510]\n",
      "loss: 2.583396  [ 1280/15510]\n",
      "loss: 2.558678  [ 1920/15510]\n",
      "loss: 2.542868  [ 2560/15510]\n",
      "loss: 2.573540  [ 3200/15510]\n",
      "loss: 2.575789  [ 3840/15510]\n",
      "loss: 2.581533  [ 4480/15510]\n",
      "loss: 2.614670  [ 5120/15510]\n",
      "loss: 2.560289  [ 5760/15510]\n",
      "loss: 2.588593  [ 6400/15510]\n",
      "loss: 2.578426  [ 7040/15510]\n",
      "loss: 2.614737  [ 7680/15510]\n",
      "loss: 2.618585  [ 8320/15510]\n",
      "loss: 2.573743  [ 8960/15510]\n",
      "loss: 2.569313  [ 9600/15510]\n",
      "loss: 2.616336  [10240/15510]\n",
      "loss: 2.604950  [10880/15510]\n",
      "loss: 2.610112  [11520/15510]\n",
      "loss: 2.619626  [12160/15510]\n",
      "loss: 2.575247  [12800/15510]\n",
      "loss: 2.574950  [13440/15510]\n",
      "loss: 2.593029  [14080/15510]\n",
      "loss: 2.540300  [14720/15510]\n",
      "loss: 2.605412  [15360/15510]\n",
      "loss: 2.536902  [    0/15510]\n",
      "loss: 2.545301  [  640/15510]\n",
      "loss: 2.578526  [ 1280/15510]\n",
      "loss: 2.613135  [ 1920/15510]\n",
      "loss: 2.573609  [ 2560/15510]\n",
      "loss: 2.596636  [ 3200/15510]\n",
      "loss: 2.628300  [ 3840/15510]\n",
      "loss: 2.611485  [ 4480/15510]\n",
      "loss: 2.588289  [ 5120/15510]\n",
      "loss: 2.588444  [ 5760/15510]\n",
      "loss: 2.524292  [ 6400/15510]\n",
      "loss: 2.589487  [ 7040/15510]\n",
      "loss: 2.570404  [ 7680/15510]\n",
      "loss: 2.572195  [ 8320/15510]\n",
      "loss: 2.574248  [ 8960/15510]\n",
      "loss: 2.583571  [ 9600/15510]\n",
      "loss: 2.521037  [10240/15510]\n",
      "loss: 2.554992  [10880/15510]\n",
      "loss: 2.598640  [11520/15510]\n",
      "loss: 2.620672  [12160/15510]\n",
      "loss: 2.565994  [12800/15510]\n",
      "loss: 2.564780  [13440/15510]\n",
      "loss: 2.564188  [14080/15510]\n",
      "loss: 2.559536  [14720/15510]\n",
      "loss: 2.547173  [15360/15510]\n",
      "loss: 2.572242  [    0/15510]\n",
      "loss: 2.536893  [  640/15510]\n",
      "loss: 2.582575  [ 1280/15510]\n",
      "loss: 2.569839  [ 1920/15510]\n",
      "loss: 2.648848  [ 2560/15510]\n",
      "loss: 2.524791  [ 3200/15510]\n",
      "loss: 2.598277  [ 3840/15510]\n",
      "loss: 2.558460  [ 4480/15510]\n",
      "loss: 2.630559  [ 5120/15510]\n",
      "loss: 2.550008  [ 5760/15510]\n",
      "loss: 2.644018  [ 6400/15510]\n",
      "loss: 2.608979  [ 7040/15510]\n",
      "loss: 2.565088  [ 7680/15510]\n",
      "loss: 2.522081  [ 8320/15510]\n",
      "loss: 2.721467  [ 8960/15510]\n",
      "loss: 2.631505  [ 9600/15510]\n",
      "loss: 2.520323  [10240/15510]\n",
      "loss: 2.531236  [10880/15510]\n",
      "loss: 2.558819  [11520/15510]\n",
      "loss: 2.579643  [12160/15510]\n",
      "loss: 2.571836  [12800/15510]\n",
      "loss: 2.532320  [13440/15510]\n",
      "loss: 2.573509  [14080/15510]\n",
      "loss: 2.558751  [14720/15510]\n",
      "loss: 2.639049  [15360/15510]\n",
      "loss: 2.563410  [    0/15510]\n",
      "loss: 2.578741  [  640/15510]\n",
      "loss: 2.559493  [ 1280/15510]\n",
      "loss: 2.568533  [ 1920/15510]\n",
      "loss: 2.564328  [ 2560/15510]\n",
      "loss: 2.536688  [ 3200/15510]\n",
      "loss: 2.556111  [ 3840/15510]\n",
      "loss: 2.644470  [ 4480/15510]\n",
      "loss: 2.551277  [ 5120/15510]\n",
      "loss: 2.540396  [ 5760/15510]\n",
      "loss: 2.607322  [ 6400/15510]\n",
      "loss: 2.558014  [ 7040/15510]\n",
      "loss: 2.587582  [ 7680/15510]\n",
      "loss: 2.573794  [ 8320/15510]\n",
      "loss: 2.612333  [ 8960/15510]\n",
      "loss: 2.592249  [ 9600/15510]\n",
      "loss: 2.556053  [10240/15510]\n",
      "loss: 2.583997  [10880/15510]\n",
      "loss: 2.571846  [11520/15510]\n",
      "loss: 2.516138  [12160/15510]\n",
      "loss: 2.597296  [12800/15510]\n",
      "loss: 2.551598  [13440/15510]\n",
      "loss: 2.563115  [14080/15510]\n",
      "loss: 2.597404  [14720/15510]\n",
      "loss: 2.570085  [15360/15510]\n",
      "loss: 2.643958  [    0/15510]\n",
      "loss: 2.534039  [  640/15510]\n",
      "loss: 2.534383  [ 1280/15510]\n",
      "loss: 2.592025  [ 1920/15510]\n",
      "loss: 2.611594  [ 2560/15510]\n",
      "loss: 2.666755  [ 3200/15510]\n",
      "loss: 2.634952  [ 3840/15510]\n",
      "loss: 2.559099  [ 4480/15510]\n",
      "loss: 2.578609  [ 5120/15510]\n",
      "loss: 2.551451  [ 5760/15510]\n",
      "loss: 2.565153  [ 6400/15510]\n",
      "loss: 2.532173  [ 7040/15510]\n",
      "loss: 2.529127  [ 7680/15510]\n",
      "loss: 2.533986  [ 8320/15510]\n",
      "loss: 2.531004  [ 8960/15510]\n",
      "loss: 2.528871  [ 9600/15510]\n",
      "loss: 2.563843  [10240/15510]\n",
      "loss: 2.571318  [10880/15510]\n",
      "loss: 2.636138  [11520/15510]\n",
      "loss: 2.618371  [12160/15510]\n",
      "loss: 2.536345  [12800/15510]\n",
      "loss: 2.568180  [13440/15510]\n",
      "loss: 2.557580  [14080/15510]\n",
      "loss: 2.682778  [14720/15510]\n",
      "loss: 2.579357  [15360/15510]\n",
      "loss: 2.605441  [    0/15510]\n",
      "loss: 2.586956  [  640/15510]\n",
      "loss: 2.604263  [ 1280/15510]\n",
      "loss: 2.567345  [ 1920/15510]\n",
      "loss: 2.614015  [ 2560/15510]\n",
      "loss: 2.578088  [ 3200/15510]\n",
      "loss: 2.650417  [ 3840/15510]\n",
      "loss: 2.564686  [ 4480/15510]\n",
      "loss: 2.598004  [ 5120/15510]\n",
      "loss: 2.592798  [ 5760/15510]\n",
      "loss: 2.570066  [ 6400/15510]\n",
      "loss: 2.572893  [ 7040/15510]\n",
      "loss: 2.569347  [ 7680/15510]\n",
      "loss: 2.559186  [ 8320/15510]\n",
      "loss: 2.557731  [ 8960/15510]\n",
      "loss: 2.525476  [ 9600/15510]\n",
      "loss: 2.628284  [10240/15510]\n",
      "loss: 2.524608  [10880/15510]\n",
      "loss: 2.579127  [11520/15510]\n",
      "loss: 2.534513  [12160/15510]\n",
      "loss: 2.638891  [12800/15510]\n",
      "loss: 2.604878  [13440/15510]\n",
      "loss: 2.582759  [14080/15510]\n",
      "loss: 2.576540  [14720/15510]\n",
      "loss: 2.556121  [15360/15510]\n",
      "loss: 2.614837  [    0/15510]\n",
      "loss: 2.567233  [  640/15510]\n",
      "loss: 2.525723  [ 1280/15510]\n",
      "loss: 2.552896  [ 1920/15510]\n",
      "loss: 2.565415  [ 2560/15510]\n",
      "loss: 2.554591  [ 3200/15510]\n",
      "loss: 2.588130  [ 3840/15510]\n",
      "loss: 2.555565  [ 4480/15510]\n",
      "loss: 2.531607  [ 5120/15510]\n",
      "loss: 2.587630  [ 5760/15510]\n",
      "loss: 2.588818  [ 6400/15510]\n",
      "loss: 2.604230  [ 7040/15510]\n",
      "loss: 2.587526  [ 7680/15510]\n",
      "loss: 2.547221  [ 8320/15510]\n",
      "loss: 2.552793  [ 8960/15510]\n",
      "loss: 2.595330  [ 9600/15510]\n",
      "loss: 2.532475  [10240/15510]\n",
      "loss: 2.570267  [10880/15510]\n",
      "loss: 2.572531  [11520/15510]\n",
      "loss: 2.580674  [12160/15510]\n",
      "loss: 2.574544  [12800/15510]\n",
      "loss: 2.689756  [13440/15510]\n",
      "loss: 2.633183  [14080/15510]\n",
      "loss: 2.573979  [14720/15510]\n",
      "loss: 2.558089  [15360/15510]\n",
      "loss: 2.536849  [    0/15510]\n",
      "loss: 2.528541  [  640/15510]\n",
      "loss: 2.549048  [ 1280/15510]\n",
      "loss: 2.536333  [ 1920/15510]\n",
      "loss: 2.549848  [ 2560/15510]\n",
      "loss: 2.572317  [ 3200/15510]\n",
      "loss: 2.560887  [ 3840/15510]\n",
      "loss: 2.555680  [ 4480/15510]\n",
      "loss: 2.566388  [ 5120/15510]\n",
      "loss: 2.585435  [ 5760/15510]\n",
      "loss: 2.530192  [ 6400/15510]\n",
      "loss: 2.578987  [ 7040/15510]\n",
      "loss: 2.593339  [ 7680/15510]\n",
      "loss: 2.600303  [ 8320/15510]\n",
      "loss: 2.600369  [ 8960/15510]\n",
      "loss: 2.572914  [ 9600/15510]\n",
      "loss: 2.583737  [10240/15510]\n",
      "loss: 2.525776  [10880/15510]\n",
      "loss: 2.573082  [11520/15510]\n",
      "loss: 2.576608  [12160/15510]\n",
      "loss: 2.546401  [12800/15510]\n",
      "loss: 2.538846  [13440/15510]\n",
      "loss: 2.544878  [14080/15510]\n",
      "loss: 2.642334  [14720/15510]\n",
      "loss: 2.589586  [15360/15510]\n",
      "loss: 2.545014  [    0/15510]\n",
      "loss: 2.588665  [  640/15510]\n",
      "loss: 2.561747  [ 1280/15510]\n",
      "loss: 2.591858  [ 1920/15510]\n",
      "loss: 2.589538  [ 2560/15510]\n",
      "loss: 2.543928  [ 3200/15510]\n",
      "loss: 2.657658  [ 3840/15510]\n",
      "loss: 2.544434  [ 4480/15510]\n",
      "loss: 2.623416  [ 5120/15510]\n",
      "loss: 2.531586  [ 5760/15510]\n",
      "loss: 2.625714  [ 6400/15510]\n",
      "loss: 2.551294  [ 7040/15510]\n",
      "loss: 2.576916  [ 7680/15510]\n",
      "loss: 2.564904  [ 8320/15510]\n",
      "loss: 2.619725  [ 8960/15510]\n",
      "loss: 2.574149  [ 9600/15510]\n",
      "loss: 2.523727  [10240/15510]\n",
      "loss: 2.549765  [10880/15510]\n",
      "loss: 2.654624  [11520/15510]\n",
      "loss: 2.583341  [12160/15510]\n",
      "loss: 2.580240  [12800/15510]\n",
      "loss: 2.592668  [13440/15510]\n",
      "loss: 2.575897  [14080/15510]\n",
      "loss: 2.586308  [14720/15510]\n",
      "loss: 2.576061  [15360/15510]\n",
      "loss: 2.566093  [    0/15510]\n",
      "loss: 2.528692  [  640/15510]\n",
      "loss: 2.553009  [ 1280/15510]\n",
      "loss: 2.549384  [ 1920/15510]\n",
      "loss: 2.601454  [ 2560/15510]\n",
      "loss: 2.603746  [ 3200/15510]\n",
      "loss: 2.532825  [ 3840/15510]\n",
      "loss: 2.579452  [ 4480/15510]\n",
      "loss: 2.584283  [ 5120/15510]\n",
      "loss: 2.542988  [ 5760/15510]\n",
      "loss: 2.558342  [ 6400/15510]\n",
      "loss: 2.565267  [ 7040/15510]\n",
      "loss: 2.626894  [ 7680/15510]\n",
      "loss: 2.578888  [ 8320/15510]\n",
      "loss: 2.546421  [ 8960/15510]\n",
      "loss: 2.564792  [ 9600/15510]\n",
      "loss: 2.575591  [10240/15510]\n",
      "loss: 2.532156  [10880/15510]\n",
      "loss: 2.595506  [11520/15510]\n",
      "loss: 2.555800  [12160/15510]\n",
      "loss: 2.559831  [12800/15510]\n",
      "loss: 2.573291  [13440/15510]\n",
      "loss: 2.577793  [14080/15510]\n",
      "loss: 2.566729  [14720/15510]\n",
      "loss: 2.542103  [15360/15510]\n",
      "loss: 2.602041  [    0/15510]\n",
      "loss: 2.579687  [  640/15510]\n",
      "loss: 2.623395  [ 1280/15510]\n",
      "loss: 2.588508  [ 1920/15510]\n",
      "loss: 2.551782  [ 2560/15510]\n",
      "loss: 2.562788  [ 3200/15510]\n",
      "loss: 2.531831  [ 3840/15510]\n",
      "loss: 2.537053  [ 4480/15510]\n",
      "loss: 2.549501  [ 5120/15510]\n",
      "loss: 2.587657  [ 5760/15510]\n",
      "loss: 2.616899  [ 6400/15510]\n",
      "loss: 2.539464  [ 7040/15510]\n",
      "loss: 2.555422  [ 7680/15510]\n",
      "loss: 2.548760  [ 8320/15510]\n",
      "loss: 2.536191  [ 8960/15510]\n",
      "loss: 2.652208  [ 9600/15510]\n",
      "loss: 2.591220  [10240/15510]\n",
      "loss: 2.553693  [10880/15510]\n",
      "loss: 2.591700  [11520/15510]\n",
      "loss: 2.706927  [12160/15510]\n",
      "loss: 2.546760  [12800/15510]\n",
      "loss: 2.561467  [13440/15510]\n",
      "loss: 2.516454  [14080/15510]\n",
      "loss: 2.562160  [14720/15510]\n",
      "loss: 2.562389  [15360/15510]\n",
      "loss: 2.621047  [    0/15510]\n",
      "loss: 2.569493  [  640/15510]\n",
      "loss: 2.605523  [ 1280/15510]\n",
      "loss: 2.609545  [ 1920/15510]\n",
      "loss: 2.527560  [ 2560/15510]\n",
      "loss: 2.622150  [ 3200/15510]\n",
      "loss: 2.583315  [ 3840/15510]\n",
      "loss: 2.586727  [ 4480/15510]\n",
      "loss: 2.596509  [ 5120/15510]\n",
      "loss: 2.569808  [ 5760/15510]\n",
      "loss: 2.643353  [ 6400/15510]\n",
      "loss: 2.573938  [ 7040/15510]\n",
      "loss: 2.527694  [ 7680/15510]\n",
      "loss: 2.584963  [ 8320/15510]\n",
      "loss: 2.556890  [ 8960/15510]\n",
      "loss: 2.617197  [ 9600/15510]\n",
      "loss: 2.594487  [10240/15510]\n",
      "loss: 2.554218  [10880/15510]\n",
      "loss: 2.599811  [11520/15510]\n",
      "loss: 2.567266  [12160/15510]\n",
      "loss: 2.551381  [12800/15510]\n",
      "loss: 2.565899  [13440/15510]\n",
      "loss: 2.631663  [14080/15510]\n",
      "loss: 2.569716  [14720/15510]\n",
      "loss: 2.562002  [15360/15510]\n",
      "loss: 2.567209  [    0/15510]\n",
      "loss: 2.651424  [  640/15510]\n",
      "loss: 2.563457  [ 1280/15510]\n",
      "loss: 2.552739  [ 1920/15510]\n",
      "loss: 2.527558  [ 2560/15510]\n",
      "loss: 2.517677  [ 3200/15510]\n",
      "loss: 2.594855  [ 3840/15510]\n",
      "loss: 2.598000  [ 4480/15510]\n",
      "loss: 2.552685  [ 5120/15510]\n",
      "loss: 2.585691  [ 5760/15510]\n",
      "loss: 2.573234  [ 6400/15510]\n",
      "loss: 2.564717  [ 7040/15510]\n",
      "loss: 2.593887  [ 7680/15510]\n",
      "loss: 2.544760  [ 8320/15510]\n",
      "loss: 2.536822  [ 8960/15510]\n",
      "loss: 2.586486  [ 9600/15510]\n",
      "loss: 2.581703  [10240/15510]\n",
      "loss: 2.593279  [10880/15510]\n",
      "loss: 2.580438  [11520/15510]\n",
      "loss: 2.555377  [12160/15510]\n",
      "loss: 2.564229  [12800/15510]\n",
      "loss: 2.578743  [13440/15510]\n",
      "loss: 2.626645  [14080/15510]\n",
      "loss: 2.540893  [14720/15510]\n",
      "loss: 2.531943  [15360/15510]\n",
      "loss: 2.547344  [    0/15510]\n",
      "loss: 2.570560  [  640/15510]\n",
      "loss: 2.585890  [ 1280/15510]\n",
      "loss: 2.574131  [ 1920/15510]\n",
      "loss: 2.603104  [ 2560/15510]\n",
      "loss: 2.571890  [ 3200/15510]\n",
      "loss: 2.565768  [ 3840/15510]\n",
      "loss: 2.602937  [ 4480/15510]\n",
      "loss: 2.555949  [ 5120/15510]\n",
      "loss: 2.645838  [ 5760/15510]\n",
      "loss: 2.575229  [ 6400/15510]\n",
      "loss: 2.581587  [ 7040/15510]\n",
      "loss: 2.569474  [ 7680/15510]\n",
      "loss: 2.577910  [ 8320/15510]\n",
      "loss: 2.562680  [ 8960/15510]\n",
      "loss: 2.565340  [ 9600/15510]\n",
      "loss: 2.595583  [10240/15510]\n",
      "loss: 2.597498  [10880/15510]\n",
      "loss: 2.613325  [11520/15510]\n",
      "loss: 2.624373  [12160/15510]\n",
      "loss: 2.559373  [12800/15510]\n",
      "loss: 2.593176  [13440/15510]\n",
      "loss: 2.612216  [14080/15510]\n",
      "loss: 2.554800  [14720/15510]\n",
      "loss: 2.556401  [15360/15510]\n",
      "loss: 2.552767  [    0/15510]\n",
      "loss: 2.519687  [  640/15510]\n",
      "loss: 2.541419  [ 1280/15510]\n",
      "loss: 2.568657  [ 1920/15510]\n",
      "loss: 2.646801  [ 2560/15510]\n",
      "loss: 2.574070  [ 3200/15510]\n",
      "loss: 2.657657  [ 3840/15510]\n",
      "loss: 2.601532  [ 4480/15510]\n",
      "loss: 2.574188  [ 5120/15510]\n",
      "loss: 2.561069  [ 5760/15510]\n",
      "loss: 2.560582  [ 6400/15510]\n",
      "loss: 2.566380  [ 7040/15510]\n",
      "loss: 2.573831  [ 7680/15510]\n",
      "loss: 2.559595  [ 8320/15510]\n",
      "loss: 2.560120  [ 8960/15510]\n",
      "loss: 2.569005  [ 9600/15510]\n",
      "loss: 2.550569  [10240/15510]\n",
      "loss: 2.550036  [10880/15510]\n",
      "loss: 2.601420  [11520/15510]\n",
      "loss: 2.579412  [12160/15510]\n",
      "loss: 2.583179  [12800/15510]\n",
      "loss: 2.546310  [13440/15510]\n",
      "loss: 2.542717  [14080/15510]\n",
      "loss: 2.554398  [14720/15510]\n",
      "loss: 2.619896  [15360/15510]\n",
      "loss: 2.578516  [    0/15510]\n",
      "loss: 2.545281  [  640/15510]\n",
      "loss: 2.568213  [ 1280/15510]\n",
      "loss: 2.653709  [ 1920/15510]\n",
      "loss: 2.539193  [ 2560/15510]\n",
      "loss: 2.548824  [ 3200/15510]\n",
      "loss: 2.578149  [ 3840/15510]\n",
      "loss: 2.591091  [ 4480/15510]\n",
      "loss: 2.565904  [ 5120/15510]\n",
      "loss: 2.545683  [ 5760/15510]\n",
      "loss: 2.572185  [ 6400/15510]\n",
      "loss: 2.579819  [ 7040/15510]\n",
      "loss: 2.569934  [ 7680/15510]\n",
      "loss: 2.596369  [ 8320/15510]\n",
      "loss: 2.613159  [ 8960/15510]\n",
      "loss: 2.560749  [ 9600/15510]\n",
      "loss: 2.557435  [10240/15510]\n",
      "loss: 2.549549  [10880/15510]\n",
      "loss: 2.601550  [11520/15510]\n",
      "loss: 2.550616  [12160/15510]\n",
      "loss: 2.621022  [12800/15510]\n",
      "loss: 2.584125  [13440/15510]\n",
      "loss: 2.530070  [14080/15510]\n",
      "loss: 2.589490  [14720/15510]\n",
      "loss: 2.568030  [15360/15510]\n",
      "loss: 2.557099  [    0/15510]\n",
      "loss: 2.578936  [  640/15510]\n",
      "loss: 2.560914  [ 1280/15510]\n",
      "loss: 2.593477  [ 1920/15510]\n",
      "loss: 2.576151  [ 2560/15510]\n",
      "loss: 2.581896  [ 3200/15510]\n",
      "loss: 2.616038  [ 3840/15510]\n",
      "loss: 2.534753  [ 4480/15510]\n",
      "loss: 2.581084  [ 5120/15510]\n",
      "loss: 2.589736  [ 5760/15510]\n",
      "loss: 2.644980  [ 6400/15510]\n",
      "loss: 2.549988  [ 7040/15510]\n",
      "loss: 2.662420  [ 7680/15510]\n",
      "loss: 2.596366  [ 8320/15510]\n",
      "loss: 2.606582  [ 8960/15510]\n",
      "loss: 2.572118  [ 9600/15510]\n",
      "loss: 2.545455  [10240/15510]\n",
      "loss: 2.618335  [10880/15510]\n",
      "loss: 2.628165  [11520/15510]\n",
      "loss: 2.628438  [12160/15510]\n",
      "loss: 2.539610  [12800/15510]\n",
      "loss: 2.543561  [13440/15510]\n",
      "loss: 2.563436  [14080/15510]\n",
      "loss: 2.595085  [14720/15510]\n",
      "loss: 2.551040  [15360/15510]\n",
      "loss: 2.561925  [    0/15510]\n",
      "loss: 2.617024  [  640/15510]\n",
      "loss: 2.565034  [ 1280/15510]\n",
      "loss: 2.567783  [ 1920/15510]\n",
      "loss: 2.625652  [ 2560/15510]\n",
      "loss: 2.643361  [ 3200/15510]\n",
      "loss: 2.611284  [ 3840/15510]\n",
      "loss: 2.548972  [ 4480/15510]\n",
      "loss: 2.563426  [ 5120/15510]\n",
      "loss: 2.586117  [ 5760/15510]\n",
      "loss: 2.571861  [ 6400/15510]\n",
      "loss: 2.568938  [ 7040/15510]\n",
      "loss: 2.600714  [ 7680/15510]\n",
      "loss: 2.542754  [ 8320/15510]\n",
      "loss: 2.567326  [ 8960/15510]\n",
      "loss: 2.524151  [ 9600/15510]\n",
      "loss: 2.556573  [10240/15510]\n",
      "loss: 2.544825  [10880/15510]\n",
      "loss: 2.559205  [11520/15510]\n",
      "loss: 2.572394  [12160/15510]\n",
      "loss: 2.662519  [12800/15510]\n",
      "loss: 2.595894  [13440/15510]\n",
      "loss: 2.570319  [14080/15510]\n",
      "loss: 2.618184  [14720/15510]\n",
      "loss: 2.615913  [15360/15510]\n",
      "loss: 2.536262  [    0/15510]\n",
      "loss: 2.580410  [  640/15510]\n",
      "loss: 2.554782  [ 1280/15510]\n",
      "loss: 2.561092  [ 1920/15510]\n",
      "loss: 2.571703  [ 2560/15510]\n",
      "loss: 2.537734  [ 3200/15510]\n",
      "loss: 2.597789  [ 3840/15510]\n",
      "loss: 2.563964  [ 4480/15510]\n",
      "loss: 2.558985  [ 5120/15510]\n",
      "loss: 2.575734  [ 5760/15510]\n",
      "loss: 2.544187  [ 6400/15510]\n",
      "loss: 2.553206  [ 7040/15510]\n",
      "loss: 2.547808  [ 7680/15510]\n",
      "loss: 2.539697  [ 8320/15510]\n",
      "loss: 2.534294  [ 8960/15510]\n",
      "loss: 2.549860  [ 9600/15510]\n",
      "loss: 2.638731  [10240/15510]\n",
      "loss: 2.591685  [10880/15510]\n",
      "loss: 2.563222  [11520/15510]\n",
      "loss: 2.522363  [12160/15510]\n",
      "loss: 2.570505  [12800/15510]\n",
      "loss: 2.550597  [13440/15510]\n",
      "loss: 2.585517  [14080/15510]\n",
      "loss: 2.549242  [14720/15510]\n",
      "loss: 2.573569  [15360/15510]\n",
      "loss: 2.556064  [    0/15510]\n",
      "loss: 2.633985  [  640/15510]\n",
      "loss: 2.514729  [ 1280/15510]\n",
      "loss: 2.535694  [ 1920/15510]\n",
      "loss: 2.543359  [ 2560/15510]\n",
      "loss: 2.576580  [ 3200/15510]\n",
      "loss: 2.548892  [ 3840/15510]\n",
      "loss: 2.606764  [ 4480/15510]\n",
      "loss: 2.530074  [ 5120/15510]\n",
      "loss: 2.552924  [ 5760/15510]\n",
      "loss: 2.598501  [ 6400/15510]\n",
      "loss: 2.563376  [ 7040/15510]\n",
      "loss: 2.528013  [ 7680/15510]\n",
      "loss: 2.578458  [ 8320/15510]\n",
      "loss: 2.540820  [ 8960/15510]\n",
      "loss: 2.591228  [ 9600/15510]\n",
      "loss: 2.527163  [10240/15510]\n",
      "loss: 2.524240  [10880/15510]\n",
      "loss: 2.565671  [11520/15510]\n",
      "loss: 2.603113  [12160/15510]\n",
      "loss: 2.556752  [12800/15510]\n",
      "loss: 2.635822  [13440/15510]\n",
      "loss: 2.517220  [14080/15510]\n",
      "loss: 2.581509  [14720/15510]\n",
      "loss: 2.568987  [15360/15510]\n",
      "loss: 2.524809  [    0/15510]\n",
      "loss: 2.566592  [  640/15510]\n",
      "loss: 2.528283  [ 1280/15510]\n",
      "loss: 2.545116  [ 1920/15510]\n",
      "loss: 2.585518  [ 2560/15510]\n",
      "loss: 2.565627  [ 3200/15510]\n",
      "loss: 2.581100  [ 3840/15510]\n",
      "loss: 2.552891  [ 4480/15510]\n",
      "loss: 2.564795  [ 5120/15510]\n",
      "loss: 2.566131  [ 5760/15510]\n",
      "loss: 2.524526  [ 6400/15510]\n",
      "loss: 2.586354  [ 7040/15510]\n",
      "loss: 2.581715  [ 7680/15510]\n",
      "loss: 2.585165  [ 8320/15510]\n",
      "loss: 2.560135  [ 8960/15510]\n",
      "loss: 2.566105  [ 9600/15510]\n",
      "loss: 2.565994  [10240/15510]\n",
      "loss: 2.566927  [10880/15510]\n",
      "loss: 2.607485  [11520/15510]\n",
      "loss: 2.589915  [12160/15510]\n",
      "loss: 2.587047  [12800/15510]\n",
      "loss: 2.653407  [13440/15510]\n",
      "loss: 2.532918  [14080/15510]\n",
      "loss: 2.557714  [14720/15510]\n",
      "loss: 2.566077  [15360/15510]\n",
      "loss: 2.531841  [    0/15510]\n",
      "loss: 2.562855  [  640/15510]\n",
      "loss: 2.578026  [ 1280/15510]\n",
      "loss: 2.546860  [ 1920/15510]\n",
      "loss: 2.541645  [ 2560/15510]\n",
      "loss: 2.527341  [ 3200/15510]\n",
      "loss: 2.552960  [ 3840/15510]\n",
      "loss: 2.609704  [ 4480/15510]\n",
      "loss: 2.546082  [ 5120/15510]\n",
      "loss: 2.572916  [ 5760/15510]\n",
      "loss: 2.530028  [ 6400/15510]\n",
      "loss: 2.550000  [ 7040/15510]\n",
      "loss: 2.555057  [ 7680/15510]\n",
      "loss: 2.544086  [ 8320/15510]\n",
      "loss: 2.548041  [ 8960/15510]\n",
      "loss: 2.581563  [ 9600/15510]\n",
      "loss: 2.574006  [10240/15510]\n",
      "loss: 2.557175  [10880/15510]\n",
      "loss: 2.522504  [11520/15510]\n",
      "loss: 2.699209  [12160/15510]\n",
      "loss: 2.553658  [12800/15510]\n",
      "loss: 2.522043  [13440/15510]\n",
      "loss: 2.558424  [14080/15510]\n",
      "loss: 2.539449  [14720/15510]\n",
      "loss: 2.577926  [15360/15510]\n",
      "loss: 2.533593  [    0/15510]\n",
      "loss: 2.586608  [  640/15510]\n",
      "loss: 2.543915  [ 1280/15510]\n",
      "loss: 2.647965  [ 1920/15510]\n",
      "loss: 2.561070  [ 2560/15510]\n",
      "loss: 2.611369  [ 3200/15510]\n",
      "loss: 2.570676  [ 3840/15510]\n",
      "loss: 2.567312  [ 4480/15510]\n",
      "loss: 2.550196  [ 5120/15510]\n",
      "loss: 2.558129  [ 5760/15510]\n",
      "loss: 2.520929  [ 6400/15510]\n",
      "loss: 2.519907  [ 7040/15510]\n",
      "loss: 2.534708  [ 7680/15510]\n",
      "loss: 2.523698  [ 8320/15510]\n",
      "loss: 2.596608  [ 8960/15510]\n",
      "loss: 2.547170  [ 9600/15510]\n",
      "loss: 2.590905  [10240/15510]\n",
      "loss: 2.568064  [10880/15510]\n",
      "loss: 2.513243  [11520/15510]\n",
      "loss: 2.545777  [12160/15510]\n",
      "loss: 2.602819  [12800/15510]\n",
      "loss: 2.542316  [13440/15510]\n",
      "loss: 2.524146  [14080/15510]\n",
      "loss: 2.606670  [14720/15510]\n",
      "loss: 2.601759  [15360/15510]\n",
      "loss: 2.530465  [    0/15510]\n",
      "loss: 2.549608  [  640/15510]\n",
      "loss: 2.570402  [ 1280/15510]\n",
      "loss: 2.581936  [ 1920/15510]\n",
      "loss: 2.516046  [ 2560/15510]\n",
      "loss: 2.558900  [ 3200/15510]\n",
      "loss: 2.559979  [ 3840/15510]\n",
      "loss: 2.509120  [ 4480/15510]\n",
      "loss: 2.558967  [ 5120/15510]\n",
      "loss: 2.554123  [ 5760/15510]\n",
      "loss: 2.572164  [ 6400/15510]\n",
      "loss: 2.509470  [ 7040/15510]\n",
      "loss: 2.547184  [ 7680/15510]\n",
      "loss: 2.562892  [ 8320/15510]\n",
      "loss: 2.545147  [ 8960/15510]\n",
      "loss: 2.517731  [ 9600/15510]\n",
      "loss: 2.555421  [10240/15510]\n",
      "loss: 2.538656  [10880/15510]\n",
      "loss: 2.599552  [11520/15510]\n",
      "loss: 2.558979  [12160/15510]\n",
      "loss: 2.547332  [12800/15510]\n",
      "loss: 2.597695  [13440/15510]\n",
      "loss: 2.549334  [14080/15510]\n",
      "loss: 2.516230  [14720/15510]\n",
      "loss: 2.513830  [15360/15510]\n",
      "loss: 2.569485  [    0/15510]\n",
      "loss: 2.544719  [  640/15510]\n",
      "loss: 2.568990  [ 1280/15510]\n",
      "loss: 2.548506  [ 1920/15510]\n",
      "loss: 2.553370  [ 2560/15510]\n",
      "loss: 2.533690  [ 3200/15510]\n",
      "loss: 2.521327  [ 3840/15510]\n",
      "loss: 2.534410  [ 4480/15510]\n",
      "loss: 2.600395  [ 5120/15510]\n",
      "loss: 2.533383  [ 5760/15510]\n",
      "loss: 2.635069  [ 6400/15510]\n",
      "loss: 2.568712  [ 7040/15510]\n",
      "loss: 2.533238  [ 7680/15510]\n",
      "loss: 2.523553  [ 8320/15510]\n",
      "loss: 2.547812  [ 8960/15510]\n",
      "loss: 2.570879  [ 9600/15510]\n",
      "loss: 2.608908  [10240/15510]\n",
      "loss: 2.582342  [10880/15510]\n",
      "loss: 2.521112  [11520/15510]\n",
      "loss: 2.552267  [12160/15510]\n",
      "loss: 2.568612  [12800/15510]\n",
      "loss: 2.533360  [13440/15510]\n",
      "loss: 2.512886  [14080/15510]\n",
      "loss: 2.498356  [14720/15510]\n",
      "loss: 2.514953  [15360/15510]\n",
      "loss: 2.595718  [    0/15510]\n",
      "loss: 2.569729  [  640/15510]\n",
      "loss: 2.559788  [ 1280/15510]\n",
      "loss: 2.573211  [ 1920/15510]\n",
      "loss: 2.528425  [ 2560/15510]\n",
      "loss: 2.522649  [ 3200/15510]\n",
      "loss: 2.568760  [ 3840/15510]\n",
      "loss: 2.562107  [ 4480/15510]\n",
      "loss: 2.579482  [ 5120/15510]\n",
      "loss: 2.553899  [ 5760/15510]\n",
      "loss: 2.600015  [ 6400/15510]\n",
      "loss: 2.526267  [ 7040/15510]\n",
      "loss: 2.574686  [ 7680/15510]\n",
      "loss: 2.526832  [ 8320/15510]\n",
      "loss: 2.585010  [ 8960/15510]\n",
      "loss: 2.613476  [ 9600/15510]\n",
      "loss: 2.544280  [10240/15510]\n",
      "loss: 2.556789  [10880/15510]\n",
      "loss: 2.536992  [11520/15510]\n",
      "loss: 2.533942  [12160/15510]\n",
      "loss: 2.560049  [12800/15510]\n",
      "loss: 2.518411  [13440/15510]\n",
      "loss: 2.553128  [14080/15510]\n",
      "loss: 2.614602  [14720/15510]\n",
      "loss: 2.569833  [15360/15510]\n",
      "loss: 2.535852  [    0/15510]\n",
      "loss: 2.552765  [  640/15510]\n",
      "loss: 2.560845  [ 1280/15510]\n",
      "loss: 2.559251  [ 1920/15510]\n",
      "loss: 2.572776  [ 2560/15510]\n",
      "loss: 2.587902  [ 3200/15510]\n",
      "loss: 2.569512  [ 3840/15510]\n",
      "loss: 2.548361  [ 4480/15510]\n",
      "loss: 2.551951  [ 5120/15510]\n",
      "loss: 2.517094  [ 5760/15510]\n",
      "loss: 2.550814  [ 6400/15510]\n",
      "loss: 2.587779  [ 7040/15510]\n",
      "loss: 2.587132  [ 7680/15510]\n",
      "loss: 2.540318  [ 8320/15510]\n",
      "loss: 2.557625  [ 8960/15510]\n",
      "loss: 2.515545  [ 9600/15510]\n",
      "loss: 2.574355  [10240/15510]\n",
      "loss: 2.542519  [10880/15510]\n",
      "loss: 2.523239  [11520/15510]\n",
      "loss: 2.563038  [12160/15510]\n",
      "loss: 2.537167  [12800/15510]\n",
      "loss: 2.533181  [13440/15510]\n",
      "loss: 2.547222  [14080/15510]\n",
      "loss: 2.618197  [14720/15510]\n",
      "loss: 2.539422  [15360/15510]\n",
      "loss: 2.579743  [    0/15510]\n",
      "loss: 2.560485  [  640/15510]\n",
      "loss: 2.622522  [ 1280/15510]\n",
      "loss: 2.568635  [ 1920/15510]\n",
      "loss: 2.587150  [ 2560/15510]\n",
      "loss: 2.536609  [ 3200/15510]\n",
      "loss: 2.550162  [ 3840/15510]\n",
      "loss: 2.579500  [ 4480/15510]\n",
      "loss: 2.606150  [ 5120/15510]\n",
      "loss: 2.543169  [ 5760/15510]\n",
      "loss: 2.559987  [ 6400/15510]\n",
      "loss: 2.568321  [ 7040/15510]\n",
      "loss: 2.632676  [ 7680/15510]\n",
      "loss: 2.515884  [ 8320/15510]\n",
      "loss: 2.535427  [ 8960/15510]\n",
      "loss: 2.543046  [ 9600/15510]\n",
      "loss: 2.620460  [10240/15510]\n",
      "loss: 2.508844  [10880/15510]\n",
      "loss: 2.562614  [11520/15510]\n",
      "loss: 2.523880  [12160/15510]\n",
      "loss: 2.573803  [12800/15510]\n",
      "loss: 2.535210  [13440/15510]\n",
      "loss: 2.526664  [14080/15510]\n",
      "loss: 2.550748  [14720/15510]\n",
      "loss: 2.564939  [15360/15510]\n",
      "loss: 2.508998  [    0/15510]\n",
      "loss: 2.575830  [  640/15510]\n",
      "loss: 2.560392  [ 1280/15510]\n",
      "loss: 2.543212  [ 1920/15510]\n",
      "loss: 2.582311  [ 2560/15510]\n",
      "loss: 2.530591  [ 3200/15510]\n",
      "loss: 2.560147  [ 3840/15510]\n",
      "loss: 2.521552  [ 4480/15510]\n",
      "loss: 2.584285  [ 5120/15510]\n",
      "loss: 2.545012  [ 5760/15510]\n",
      "loss: 2.521673  [ 6400/15510]\n",
      "loss: 2.559249  [ 7040/15510]\n",
      "loss: 2.593652  [ 7680/15510]\n",
      "loss: 2.603600  [ 8320/15510]\n",
      "loss: 2.565077  [ 8960/15510]\n",
      "loss: 2.625013  [ 9600/15510]\n",
      "loss: 2.556727  [10240/15510]\n",
      "loss: 2.548250  [10880/15510]\n",
      "loss: 2.551281  [11520/15510]\n",
      "loss: 2.536831  [12160/15510]\n",
      "loss: 2.550374  [12800/15510]\n",
      "loss: 2.535123  [13440/15510]\n",
      "loss: 2.551530  [14080/15510]\n",
      "loss: 2.546099  [14720/15510]\n",
      "loss: 2.554348  [15360/15510]\n",
      "loss: 2.511323  [    0/15510]\n",
      "loss: 2.627111  [  640/15510]\n",
      "loss: 2.525873  [ 1280/15510]\n",
      "loss: 2.573055  [ 1920/15510]\n",
      "loss: 2.576407  [ 2560/15510]\n",
      "loss: 2.551863  [ 3200/15510]\n",
      "loss: 2.521981  [ 3840/15510]\n",
      "loss: 2.589114  [ 4480/15510]\n",
      "loss: 2.575253  [ 5120/15510]\n",
      "loss: 2.587810  [ 5760/15510]\n",
      "loss: 2.522629  [ 6400/15510]\n",
      "loss: 2.544725  [ 7040/15510]\n",
      "loss: 2.567690  [ 7680/15510]\n",
      "loss: 2.566532  [ 8320/15510]\n",
      "loss: 2.601167  [ 8960/15510]\n",
      "loss: 2.546670  [ 9600/15510]\n",
      "loss: 2.631881  [10240/15510]\n",
      "loss: 2.533750  [10880/15510]\n",
      "loss: 2.532146  [11520/15510]\n",
      "loss: 2.557676  [12160/15510]\n",
      "loss: 2.566091  [12800/15510]\n",
      "loss: 2.565046  [13440/15510]\n",
      "loss: 2.605090  [14080/15510]\n",
      "loss: 2.527938  [14720/15510]\n",
      "loss: 2.557383  [15360/15510]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    contrastive_train_loop(dl, model, loss_fn, optimizer)\n",
    "\n",
    "torch.save(model, './models/contrastive_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257343aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch-ni-ny]",
   "language": "python",
   "name": "conda-env-torch-ni-ny-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
